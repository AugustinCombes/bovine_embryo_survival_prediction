{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td align=\"left\"; style=\"background-color:transparent; width: 50%;\"><a href=\"https://www.inrae.fr\"><img src=\"https://www.inrae.fr/themes/custom/inrae_socle/logo.svg\" width=\"30%\"></a></td>\n",
    "    <td align=\"right\"; style=\"background-color:transparent; width: 50%;\"><a href = \"https://www.vet-alfort.fr/\"><img src=\"https://www.vet-alfort.fr/images/logo-enva.svg\" width=\"30%\"></a></td>\n",
    "  </tr>\n",
    "</table> \n",
    "</div>\n",
    "\n",
    "<center><h1>Bovine Embryos Movies challenge</h1></center>\n",
    "\n",
    "<center><h3>A data challenge on early prediction of the fate of bovine embryos</h3></center>\n",
    "<br/>\n",
    "<center><i>Julien Chiquet (MIA Paris-Saclay, Inrae), Pierre Gloaguen (MIA Paris-Saclay, AgroParisTech), Nicolas Jouvin (MIA Paris-Saclay), Patrick Bouthemy (SERPICO, Inria), Alain Truibil (MaiAGE, Inrae), Alline Reis (PASP, ENVA) </i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective of the challenge\n",
    "\n",
    "This challenge consists in predicting the state of bovine embryos as early as possible after in-vitro fertilization.\n",
    "\n",
    "We have defined 8 different classes (denoted from \"A\" to \"H\" in this challenge) corresponding to 8 different biological states from alive (\"A\") to dead (\"H\"). These labels express the state of the embryo after 8 days. However, it is of great interest to be able to predict this future state earlier. **The goal of this challenge is to make this prediction between 1 and 4 days.**\n",
    "\n",
    "For this, you have access to a set of videos (timelapses) of bovine embryo development for a total of 300 snapshots taken every fifteen minutes.\n",
    "\n",
    "> *Note:* Due to experimental conditions, the first snapshot was acquired at a time $t_0$ that might vary between videos. The timestamp information of each frame in each video is available and accessible. The designed model will aim at predicting at specific *times* (and not frame numbers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The following cell will install the required package dependencies, if necessary. You can examine the file, `requirements.txt`, included in the repo to view the list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** Due to the structure of the challenge, libraries not included in `requirements.txt` will need to be added to be added via a pull request to the [GitHub repo](https://github.com/ramp-kits/bovinsembryos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `ramp-workflow` package from the master branch on GitHub using the following command in you dedicated python environment."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/paris-saclay-cds/ramp-workflow.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem.py file\n",
    "\n",
    "This file contains the definition of the data-challenge according to the RAMP framework. In addition, it contains some useful methods and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data, get_test_data, WeightedClassificationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "The public data are stored in a public OSF repository, you only need to run the following script which create the `data/` repository locally.\n",
    "\n",
    "> Note that, in order to be registered to the RAMP studio event, partiticipants must fill the [following form](https://framaforms.org/access-request-to-the-bovmovies2pred-data-challenge-osf-dataset-1646386323) which contain an agreement on the use of the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration <a name=data_exp></a>\n",
    "\n",
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data using the utility function designed for the challenge in `problem.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_train, labels_train = get_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`labels_trains` simply consists in the set of labels for the training set, stored as a `numpy.darray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_train[:10])\n",
    "print(labels_train.shape)\n",
    "print('Number of videos in the training set: {}'.format(labels_train.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand `videos_train` is a list of objects from the class `VideoReader`. This class is fully described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(videos_train)) # List\n",
    "print(type(videos_train[0])) # VideoReader object (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization \n",
    "\n",
    "A video is timelapse of the embryos development, with one image every 15 minutes. One may plot a video image at a specific time (expressed in hour) via the `read_frame` helper method (described below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Here, we use the read_frame method to extract a specific time\n",
    "# See below for the description of this method\n",
    "plt.imshow(videos_train[0].read_frame(frame_time=24), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may also inspect the class distribution among the 8 possible categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs, counts = np.unique(labels_train, return_counts=True)\n",
    "plt.bar(labs, counts)\n",
    "plt.title(\"Barplot of class distribution in the train dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VideoReader class\n",
    "\n",
    "The `VideoReader` class aims at manipulating the videos. It is basically a wrapper for some openCV features of the library `cv2`, and its main aim is to ease the manipulation of videos for the participants. It is implemented in the `problem.py` file, still we copy its definition here for the sake of self-completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can import the VideoReader class from problem.py via\n",
    "from problem import VideoReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class VideoReader:\n",
    "    def __init__(self, video_filename, frame_times, img_size=[250, 250]):\n",
    "        self.video = cv2.VideoCapture(video_filename)\n",
    "        self.nb_frames = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.img_size = img_size\n",
    "        self.frame_times = frame_times\n",
    "\n",
    "    def read_frame(self, frame_time):\n",
    "        \"\"\"Return the frame of a VideoReader object at the specified `frame_time`\n",
    "\n",
    "        Args:\n",
    "            frame_time (float): the specified time in hours (allowing quarter hours, e.g. 25.75 or 26.50)\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the specified time does not exist for the selected video\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2-D array containing the grayscale image. \n",
    "        \"\"\"\n",
    "        if frame_time not in self.frame_times:\n",
    "            raise ValueError('The specified frame time must me within the time '\n",
    "                             'interval of the video.')\n",
    "\n",
    "        frame_nb = np.where(self.frame_times == frame_time)[0][0]\n",
    "        if frame_nb is not None:\n",
    "            self.video.set(cv2.CAP_PROP_POS_FRAMES, frame_nb)\n",
    "        _, frame = self.video.read()\n",
    "\n",
    "        # always reset video's frame counter to 0 to avoid unexpected behavior\n",
    "        self.video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "        return cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    def read_sequence(self, begin_time=None, end_time=None):\n",
    "        \"\"\"Extract the sequence of consecutive frames from begin_time to end_time (included)\n",
    "\n",
    "        Args:\n",
    "            begin_time (float, optional): The time where the extraction begins. Defaults to None.\n",
    "            end_time (float, optional):  The time where the extraction ends. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 3-D numpy array with first axis corresponding to the frame index and the remaining dimension to image size.\n",
    "        \"\"\"\n",
    "        if begin_time is None:\n",
    "            begin_time = self.frame_times[0]\n",
    "        elif begin_time not in self.frame_times:\n",
    "            raise ValueError('The specified begin_time must me within the time'\n",
    "                             ' interval of the video.')\n",
    "\n",
    "        if end_time is None:\n",
    "            end_time = self.frame_times[-1]\n",
    "        elif end_time not in self.frame_times:\n",
    "            raise ValueError('The specified end_time must me within the time '\n",
    "                             'interval of the video.')\n",
    "\n",
    "        if begin_time > end_time:\n",
    "            raise ValueError(\"begin_time must be smaller than end_time.\")\n",
    "\n",
    "        begin_nb = np.where(self.frame_times == begin_time)[0][0]\n",
    "        end_nb = np.where(self.frame_times == end_time)[0][0]\n",
    "        self.video.set(cv2.CAP_PROP_POS_FRAMES, begin_nb)\n",
    "\n",
    "        my_frames = list(range(begin_nb, end_nb + 1))\n",
    "        video_array = np.empty(\n",
    "            shape=(len(my_frames), self.img_size[0], self.img_size[1])\n",
    "        )\n",
    "        for t, _ in enumerate(my_frames):\n",
    "            _, frame = self.video.read()\n",
    "            video_array[t, :, :] = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # always reset video's frame counter to 0 to avoid unexpected behavior \n",
    "        self.video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "        return video_array\n",
    "\n",
    "    def read_samples(self, selected_times=None):\n",
    "        \"\"\"Read several frames of the video at once corresponding to the selected times.\n",
    "\n",
    "        Args:\n",
    "            selected_times (list, optional): The list of of desired extraction times, in hours (allowing quarter hour). Defaults to None, the whole 300 frames are returned.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 3-D numpy array with of shape (size len(selected_times), 250, 250).\n",
    "        \"\"\"\n",
    "        if selected_times is None:\n",
    "            selected_times = self.frame_times\n",
    "\n",
    "        res = np.empty([len(selected_times), self.img_size[0], self.img_size[1]])\n",
    "        frame_nbs = np.where([t in selected_times for t in self.frame_times])[0]\n",
    "        for i, f in enumerate(frame_nbs):\n",
    "            self.video.set(cv2.CAP_PROP_POS_FRAMES, f)\n",
    "            _, frame = self.video.read()\n",
    "\n",
    "            res[i, :, :] = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        self.video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "The `VideoReader` class has 3 attributes illustrated in the next cell :\n",
    "- `video`: the video object, which is as cv2.VideoCapture (see [the documentation](https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html))\n",
    "- `nb_frames`: the number of frames. This should be 300 for each video.\n",
    "- `frame_times`:  a 1-D `numpy.array` containing the times at which the frames where recorded. These times are expressed in hours since the fecondation. **All frames** are separated by 15 mns (0.25 hours). However, due to different experimental conditions, videos **might start at different times**!\n",
    "- `img_size` returns the dimensions of the square images composing the video. This should be normalized to $250 \\times 250$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_video = videos_train[0]\n",
    "print(\"A video is stored as a \" + str(type(example_video.video)) + \" Python object.\")\n",
    "print(\"It has \" + str(example_video.nb_frames) + \" frames of size \" + str(example_video.img_size) + \".\")\n",
    "print(\"The frame are recorded at the following times (expressed as time in hour since the fecondation)\")\n",
    "print(example_video.frame_times[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "The `VideoReader` class has methods illustrated in the next cell :\n",
    "- The `read_frame` method returns the frame of the video at a specified time.  The user must provide the `frame_time` argument, which is a time present in the `frame_times` attribute. This method returns a 2-D `numpy.ndarray`.\n",
    "- The `read_sequence` method returns a sequence of **consecutive frames**. The user must provide the `begin_time` argument (the frame time at which the sequence begins) and the `end_time` (the frame time at which the sequence ends). By default, these two arguments are set to `None`, which results in starting at first time and ending at last time. The method returns a 3-D ($n \\times 250 \\times 250$) `numpy.ndarray`, where $n$ is the number of consecutive times between `begin_time` and `end_time` (included).\n",
    "- The `read_samples` method transforms the video to a 3-D ($300 \\times 250 \\times 250$) `numpy.ndarray`. Optionally, a `selected_times` argument (provided as a list) may be specified to only select frames corresponding to these times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_times = example_video.frame_times[[0, 200, 299]] \n",
    "print(\"Plot the videos frames corresponding to times: \" + str(selected_times) + \" hours.\")\n",
    "for frame_time in selected_times:\n",
    "    plt.imshow(example_video.read_frame(frame_time), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array = example_video.read_sequence()\n",
    "video_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array = example_video.read_sequence(end_time = 29)\n",
    "print(video_array.shape)\n",
    "\n",
    "video_array = example_video.read_sequence(begin_time = 28.75, end_time=29)\n",
    "print(video_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_array = example_video.read_samples(selected_times=[23.75, 62.25, 96])\n",
    "video_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Caution:** the VideoReader class does not load all the videos frame into memory, which is useful in order not to overload you local machine or the RAMP servers. However, the `read_sample` or `read_sequence` may quickly lead to memory issues when trying to load every videos. You code should take this issue into account and treat videos sequentially or, at least, batch-wise."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DO NOT TRY TO RUN (memory will collapse)\n",
    "data = []\n",
    "for video in videos_train:\n",
    "    data.append(data.read_samples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "For the moment, we only consider the overall classification accuracy of your classifiers.\n",
    "\n",
    "Locally, the RAMP platform use a 3-fold cross-validation scheme implemented for you in the `get_cv` method. Moreover, the classifier's performance is evaluated on a separate test set which can be loaded just as the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_test, labels_test  = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of subjects in the test set: {}'.format(labels_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling several prediction times\n",
    "\n",
    "Remember that the challenge's objective is to design classifier(s) for different prediction times between 1 and 4 days (24 to 96 hours). Your submission should account for that constraints with a `pred_time` argument for the model fit and prediction, ensuring that only frames up to `pred_times` are used for model training and testing.\n",
    "\n",
    "The predefined and fixed prediction times are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_times = [27, 32, 37, 40, 44, 48, 53, 58, 63, 94, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important:** You are freed to opt for having one model for each prediction time, or one model for every time. However, note that for the second option your model will be fully retrained at each prediction time, and that it must handle videos with a varying number of frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory structure of a submission\n",
    "\n",
    "\n",
    "A submission (usually stored in `./subsmissions/<submission_foldername>/`) must contain on file named `videoclassifier.py`.\n",
    "\n",
    "This python script must itself implement at least a `VideoClassifier` class with\n",
    " * A `fit(videos, y, pred_time)` method.\n",
    " * A `pred(videos, pred_time)` method.\n",
    "\n",
    "The two arguments must be understood as follow:\n",
    " * `videos` is a list of `VideoReader` object with videos object cut at `pred_times`.\n",
    " * `y` is a 1-D numpy array containing the associated training labels.\n",
    " * `pred_time` is the time of development for which the prediction shall be made. \n",
    "\n",
    "We illustrate this below with a simple example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Illustration : a dummy random classifier\n",
    "\n",
    "This classifier does not use (nor even load into memory) the videos or the prediction time, and just predict random labels. Still, it is a valid (albeit unuseful) submission regarding the RAMP workflow.\n",
    "\n",
    "It is implemented in the `./subsmissions/starting_kit/` folder and we copy its code here for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class VideoClassifier(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, videos: list, y, pred_time: float):\n",
    "        classes = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "        self.n_classes = len(classes)\n",
    "        pass\n",
    "\n",
    "    def predict(self, videos: list, pred_time: float):\n",
    "        proba = np.random.rand(len(videos), self.n_classes)\n",
    "        proba /= proba.sum(axis=1)[:, np.newaxis]\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pred_time = 96\n",
    "my_model = VideoClassifier()\n",
    "my_model.fit(videos_train, labels_train, pred_time=my_pred_time)\n",
    "y_pred = my_model.predict(videos_test, pred_time=my_pred_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score function\n",
    "\n",
    "For now, only weighted classification error is implemented. The LOWER the better. This score is based on weighted errors. The weights were determined by an expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lett_to_num(y):\n",
    "    \"\"\"Return a numeric encoding of the letters\"\"\"\n",
    "    return [ord(lab) - 65 for lab in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wce = WeightedClassificationError(time_idx=my_pred_time)\n",
    "wce.compute(y_true=lett_to_num(labels_test), y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_pred_time in pred_times:\n",
    "    my_model = VideoClassifier()\n",
    "    my_model.fit(videos_train, labels_train, pred_time=my_pred_time)\n",
    "    y_pred = my_model.predict(videos_test, pred_time=my_pred_time)\n",
    "    wce = WeightedClassificationError(time_idx=my_pred_time)\n",
    "    print('Classification error at time ' + str(my_pred_time) + ' is:', str(wce.compute(y_true=lett_to_num(labels_test), y_pred=y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare to the performance of a (dummy) classifier, returning only one of the 8 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cl in np.unique(labels_train):\n",
    "    y_cl = np.zeros((labels_train.size, np.unique(labels_train).size))\n",
    "    y_cl[:, lett_to_num(cl)] = 1\n",
    "    print('Classifying all videos in class', cl, 'gives a score of:', wce.compute(lett_to_num(labels_train), y_cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting to RAMP\n",
    "\n",
    "Before submitting to RAMP, you can test your solution locally to ensure that trivial errors (e.g. typos, path issues, etc.) are resolved. We can test a given submission using the `ramp` command that was installed in the virtual environment.  \n",
    "We'll use the following command:  \n",
    "`!ramp test --submission <subm_folder> --quick-test`  \n",
    "The `!` signals that the command should be run on the command line instead of this notebook.  \n",
    "`ramp-test` is the command to be executed. It signals `ramp` to perform a local test. \n",
    "`--submission <subm_folder>` specifies which submission to run. You can have multiple potential submissions in the `submissions/` directory; this prevents `ramp` from running all of them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp-test --quick-test --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are not very good, but that is expected: our estimator in starting_kit completely ignores the data and returns a random prediction!\n",
    "\n",
    "RAMP will automatically perform 3-fold cross-validation and report the WeightedClassificationError for each of the folds and each prediction times, along with the mean across the folds. Bagging of the results has been disabled; the output can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
